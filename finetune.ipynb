{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out Microsoft Phi model (Inferencing and Fine tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (0.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (2.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.17 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: filelock in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from huggingface-hub>=0.18.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (1.3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (0.25.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.17 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from accelerate) (2.3.0.dev20231215)\n",
      "Requirement already satisfied: huggingface-hub in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: filelock in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "\n",
    "%pip install einops\n",
    "%pip install datasets\n",
    "%pip install scikit-learn\n",
    "%pip install accelerate -U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model using Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omkarmore/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "model_name = \"gpt2\"\n",
    "#model_name = \"microsoft/phi-2\"\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a short childrens story. Once upon a time there was a thristy crow. The thristy crow was a tiny little animal with a human face, but no eyes or ears. Some people described it as being like a chicken, but it never came off the chicken. It was so tiny it had to burrow under the skin of a young child and live in the earth. The kid couldn't chew food and that was good news because it was such a small animal.\n",
      "\n",
      "\"In the beginning of the book he describes how his little chicken was so small, so small, that if he got too much he could crawl, but there was no way he could take his food and eat it, because there was no water inside, and there was no food. And when he died, the little chicken said, 'My life is over.' So my little bird died and didn't remember he was there. It was one of the only creatures that never grew old. There was always a problem with that one. When there is one thing you need to feed in large quantities it is always one that feeds in small amounts. And it was this that was killing you and destroying your world. If you feed lots of small items that feed in large amounts you have a problem.\"\n",
      "\n",
      "\"A couple of years later we were in a very tiny world when, in his memory, and from the inside out there was no water inside, it was dead. The little bird of the forest was a child, and what was lost was a very very small animal that could not go anywhere, and that I have never seen with this human. So I would make a few guesses and tell you what happens to the child. Now it would be quite surprising to me to make that guess in the beginning. But the child would always be so tiny and the little bird was in danger, so that would make it appear that it died, that I lost something in the process. And that's what led me to believe that the child's only source of food was food that was only found under very cold, very cold conditions.\"\n"
     ]
    }
   ],
   "source": [
    "txt = pipe('Here is a short childrens story. Once upon a time there was a thristy crow', max_length=512)[0]['generated_text']\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune a Phi model for Text Generation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a dataset (replace with your dataset)\n",
    "dataset = load_dataset(\"euclaise/writingprompts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  [ WP ] You 've finally managed to discover the...   \n",
      "1  [ WP ] The moon is actually a giant egg , and ...   \n",
      "2  [ WP ] You find a rip in time walking through ...   \n",
      "3  [ WP ] For years in your youth the same imagin...   \n",
      "4  [ WP ] You glance at your watch 10:34 am , rou...   \n",
      "\n",
      "                                               story  \n",
      "0  So many times have I walked on ruins, the rema...  \n",
      "1  -Week 18 aboard the Depth Reaver, Circa 2023- ...  \n",
      "2  I was feckin' sloshed, mate. First time I ever...  \n",
      "3  “ No, no no no... ” She backed up and turned t...  \n",
      "4  There's a magical moment between wakefulness a...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sampled_dataset = dataset['train'].select(range(5))\n",
    "df = pd.DataFrame(sampled_dataset)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15138/15138 [00:07<00:00, 2029.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate the prompt and story\n",
    "    concatenated_texts = [prompt + \" \" + story for prompt, story in zip(examples['prompt'], examples['story'])]\n",
    "    tokenized_inputs = tokenizer(concatenated_texts, padding='max_length', truncation=True, max_length=512)  # Adjust max_length as needed\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10,000 entries\n",
    "sampled_dataset_dict = {split: tokenized_datasets[split].shuffle(seed=42).select(range(100))\n",
    "                        for split in tokenized_datasets.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  Light bulbs have gotten efficient enouh to las...   \n",
      "1  [ WP ] Just write about something that makes y...   \n",
      "2  [ WP ] With the discovery of alternate dimensi...   \n",
      "3  [ WP ] A man has been waiting for his daughter...   \n",
      "4  [ EU ] Before Bruce Wayne 's death , he create...   \n",
      "\n",
      "                                               story  \\\n",
      "0  The three men stood in silence and stared up a...   \n",
      "1  I ’ ve learned to appreciate all kinds of beau...   \n",
      "2  Edit: I wrote this in mind for Blake Andersen ...   \n",
      "3  Riley sits in his padded cell with his back to...   \n",
      "4  It started when I was 8. I had been staying ov...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [15047, 34122, 423, 7891, 6942, 551, 280, 71, ...   \n",
      "1  [58, 28993, 2361, 2329, 3551, 546, 1223, 326, ...   \n",
      "2  [58, 28993, 2361, 2080, 262, 9412, 286, 13527,...   \n",
      "3  [58, 28993, 2361, 317, 582, 468, 587, 4953, 32...   \n",
      "4  [58, 4576, 2361, 7413, 11088, 13329, 705, 82, ...   \n",
      "\n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                              labels  \n",
      "0  [15047, 34122, 423, 7891, 6942, 551, 280, 71, ...  \n",
      "1  [58, 28993, 2361, 2329, 3551, 546, 1223, 326, ...  \n",
      "2  [58, 28993, 2361, 2080, 262, 9412, 286, 13527,...  \n",
      "3  [58, 28993, 2361, 317, 582, 468, 587, 4953, 32...  \n",
      "4  [58, 4576, 2361, 7413, 11088, 13329, 705, 82, ...  \n"
     ]
    }
   ],
   "source": [
    "sampled_dataset = sampled_dataset_dict['train'].select(range(5))\n",
    "df = pd.DataFrame(sampled_dataset)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./{model_name}-finetuned\",\n",
    "    per_device_train_batch_size=1,  # Adjust batch size according to your GPU memory\n",
    "   # gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['prompt', 'story', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['prompt', 'story', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['prompt', 'story', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 100\n",
       " })}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "tokenized_datasets\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=sampled_dataset_dict[\"train\"],  # Replace with your train dataset\n",
    "    eval_dataset=sampled_dataset_dict[\"validation\"]  # Replace with your validation dataset, if available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 11/300 [00:05<00:57,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0507, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 21/300 [00:06<00:47,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5772, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 31/300 [00:08<00:45,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2964, 'learning_rate': 4.5e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 41/300 [00:10<00:43,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5744, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 51/300 [00:11<00:42,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8681, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 61/300 [00:13<00:41,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0721, 'learning_rate': 4e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 71/300 [00:15<00:39,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1395, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 81/300 [00:17<00:37,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1731, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 91/300 [00:18<00:35,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1098, 'learning_rate': 3.5e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 101/300 [00:20<00:34,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2827, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 111/300 [00:22<00:32,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9675, 'learning_rate': 3.1666666666666666e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 121/300 [00:23<00:30,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1296, 'learning_rate': 3e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 131/300 [00:25<00:29,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6999, 'learning_rate': 2.8333333333333335e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 141/300 [00:27<00:26,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.009, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 151/300 [00:29<00:25,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3311, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 161/300 [00:30<00:23,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5945, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 171/300 [00:32<00:21,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9311, 'learning_rate': 2.1666666666666667e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 181/300 [00:34<00:20,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9781, 'learning_rate': 2e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 191/300 [00:35<00:18,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1184, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 201/300 [00:37<00:16,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2314, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 211/300 [00:39<00:15,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5893, 'learning_rate': 1.5e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 221/300 [00:40<00:13,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.817, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 231/300 [00:42<00:11,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9653, 'learning_rate': 1.1666666666666668e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 241/300 [00:44<00:09,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9548, 'learning_rate': 1e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 251/300 [00:45<00:08,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4405, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 261/300 [00:47<00:06,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7239, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 271/300 [00:49<00:04,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4345, 'learning_rate': 5e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 281/300 [00:51<00:03,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1011, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 291/300 [00:52<00:01,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7133, 'learning_rate': 1.6666666666666667e-06, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:54<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.681, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'train_runtime': 54.3664, 'train_samples_per_second': 5.518, 'train_steps_per_second': 5.518, 'train_loss': 2.8851855278015135, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=2.8851855278015135, metrics={'train_runtime': 54.3664, 'train_samples_per_second': 5.518, 'train_steps_per_second': 5.518, 'train_loss': 2.8851855278015135, 'epoch': 3.0})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetuned/tokenizer_config.json',\n",
       " './gpt2-finetuned/special_tokens_map.json',\n",
       " './gpt2-finetuned/vocab.json',\n",
       " './gpt2-finetuned/merges.txt',\n",
       " './gpt2-finetuned/added_tokens.json',\n",
       " './gpt2-finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(f\"./{model_name}-finetuned/\")\n",
    "tokenizer.save_pretrained(f\"./{model_name}-finetuned/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = f\"./{model_name}-finetuned\"  # Replace with your model directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = f\"./gpt2-finetuned\"\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a short childrens story. Once upon a time there was a thristy crow's nest all over the place, and all of it lay dark and silent, the sun setting a crisp green over the bright woods. The thristy crows never fell asleep. The only darkness was the trees. They grew into trees, and the thirst had given way to the light. Brightly lit, light filled the darkness. The last of the trees grew dimly, and then dark and lifeless. The trees burned and the sun lit the darkness even more. They bore faint scars and scars that they could not make out and did it all over again. Sometimes I heard whispers of the tree that burned to the ground. It was the last of the trees, and the trees that grew old were forgotten and nothing truly remained of them. The tree fell into the forest just like the last time, and then the lights of the dark had returned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt = pipe('Here is a short childrens story. Once upon a time there was a thristy crow', max_length=512)[0]['generated_text']\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
